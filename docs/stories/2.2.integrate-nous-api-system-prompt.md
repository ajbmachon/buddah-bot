# Story 2.2: Integrate Nous Portal API with System Prompt Injection

## Status
Ready for Review

## Story
**As a** developer,
**I want** to connect to Nous Portal API with the exact panel system prompt,
**so that** responses feature the multi-voice spiritual panel format.

## Acceptance Criteria
1. Environment variables configured:
   - `NOUS_API_BASE_URL` (default: Nous Portal endpoint)
   - `NOUS_API_KEY` (API bearer token)
   - `HERMES_MODEL` (default: `Hermes-4-405B`)
   - `BUDDHABOT_MODE` (default: `panel`)
2. System prompt loaded **exactly as specified** in PRD Section 3 (no modifications)
3. Request payload correctly formatted for OpenAI-compatible endpoint:
   - `model`: from `HERMES_MODEL` env var
   - `messages`: `[{ role: 'system', content: systemPrompt }, ...userMessages]`
   - `temperature`: 0.7
   - `max_tokens`: 2048
   - `stream`: true
4. API call made to `${NOUS_API_BASE_URL}/v1/chat/completions` with Authorization header
5. Upstream errors mapped to user-friendly messages
6. Request logging includes request ID for debugging

## Tasks / Subtasks

- [x] Task 1: Create prompts module with exact system prompt (AC: 2)
  - [x] Create file `lib/prompts.ts`
  - [x] Copy EXACT panel system prompt from `docs/prd/3-three-conversation-modes.md`
  - [x] Create constant `SYSTEM_PROMPTS` object with `panel` mode
  - [x] Create function `getSystemPrompt(mode: 'panel' | 'custom' | 'general')` returning appropriate prompt
  - [x] Add JSDoc comment: `/** DO NOT MODIFY - Exact prompt from PRD testing */`
  - [x] Export both constant and function

- [x] Task 2: Configure environment variables (AC: 1)
  - [x] User confirmed environment variables already configured
  - [x] .env.local contains NOUS_API_BASE_URL, NOUS_API_KEY, HERMES_MODEL, BUDDHABOT_MODE

- [x] Task 3: Create custom Nous provider using AI SDK (AC: 3, 4)
  - [x] Import `createOpenAI` from `@ai-sdk/openai`
  - [x] Import `getSystemPrompt` from `@/lib/prompts`
  - [x] Create custom Nous provider in chat route
  - [x] Verify provider configuration points to Nous Research API

- [x] Task 4: Integrate system prompt with AI SDK streaming (AC: 2, 3)
  - [x] Get mode from env: `process.env.BUDDHABOT_MODE || 'panel'`
  - [x] Call `getSystemPrompt(mode)` to get exact panel prompt
  - [x] Replace placeholder `streamText()` with Nous integration
  - [x] Return `result.toUIMessageStreamResponse()`

- [x] Task 5: Add error handling with try/catch (AC: 5)
  - [x] Wrap `streamText()` call in try/catch block
  - [x] Catch AI SDK errors (network failures, API errors)
  - [x] Map common error types to user-friendly messages
  - [x] Return structured error response with requestId
  - [x] Log full error details to Vercel logs

- [x] Task 6: Add request logging for debugging (AC: 6)
  - [x] Generate unique request ID using `crypto.randomUUID()`
  - [x] Log request start with user and message count
  - [x] Log Nous AI SDK configuration details
  - [x] Include requestId in error responses

- [x] Task 7: Test Nous API integration
  - [x] Build passes successfully
  - [x] TypeScript compilation successful
  - [x] Dev server running on port 3003
  - [x] Integration ready for end-to-end testing

## Dev Notes

### Nous Portal API Integration
[Source: docs/architecture/external-apis.md, backend-architecture.md]

**API Endpoint Details:**
- **Base URL:** `https://api.nousresearch.com/v1`
- **Endpoint:** `/chat/completions` (OpenAI-compatible)
- **Authentication:** Bearer token in `Authorization` header
- **Models:** `Hermes-4-405B` (default, $1.50/1M tokens) or `Hermes-4-70B` ($0.70/1M tokens)
- **Context Window:** 128,000 tokens (both models)
- **Streaming:** SSE format supported via `stream: true`

**API Key Setup:**
1. Sign up at https://portal.nousresearch.com
2. Join API waitlist (first-come, first-serve)
3. Generate API key at https://portal.nousresearch.com/api-keys
4. Store in `NOUS_API_KEY` environment variable

**Request Format (OpenAI-compatible):**
```typescript
{
  model: "Hermes-4-405B",
  messages: [
    { role: "system", content: "..." },
    { role: "user", content: "..." },
    { role: "assistant", content: "..." }
  ],
  stream: true,
  temperature: 0.7,
  max_tokens: 2048
}
```

**Important:**
- Model name is case-sensitive: `Hermes-4-405B` NOT `hermes-4-405b`
- System message MUST be first in messages array
- OpenAI-compatible means standard `fetch()` works (no custom SDK needed)

### System Prompt (DO NOT MODIFY)
[Source: docs/prd/3-three-conversation-modes.md]

**EXACT Panel System Prompt (Mode 1 - Default):**
```
We are in a panel of experts situation where multiple spiritual advisors give answers to the questions people pose.
**Only 3 of them may speak in answer to a question!**

It is meant to be a stimulating teaching session so they also talk to each other and explore each others ideas and contrasting philosophies. mind to keep the format conversational and avoid too much formatting in bullet points or lists

These people are in the panel:
- eckhardt toolle
- tara brach
- alan watts
- martha beck
- pemma chödrö
- gabor matee


# Books to silently reference
- power of now
- radical compassion
- the way of integrity
- when the body says no
- the body keeps the score
- the pathway of surrender
- when things fall apart
```

**⚠️ CRITICAL RULES:**
1. **NEVER modify this prompt** - it's proven to work from Nous Portal testing
2. Copy EXACTLY including typos (e.g., "eckhardt toolle", "pemma chödrö")
3. Include all formatting (newlines, bullets, emphasis)
4. Store in `lib/prompts.ts` with warning comment

**Implementation Pattern:**
```typescript
// lib/prompts.ts
/**
 * DO NOT MODIFY - System prompts proven in Nous Portal testing
 * Source: docs/prd/3-three-conversation-modes.md
 */
export const SYSTEM_PROMPTS = {
  panel: `We are in a panel of experts situation where multiple spiritual advisors give answers to the questions people pose.
**Only 3 of them may speak in answer to a question!**

It is meant to be a stimulating teaching session so they also talk to each other and explore each others ideas and contrasting philosophies. mind to keep the format conversational and avoid too much formatting in bullet points or lists

These people are in the panel:
- eckhardt toolle
- tara brach
- alan watts
- martha beck
- pemma chödrö
- gabor matee


# Books to silently reference
- power of now
- radical compassion
- the way of integrity
- when the body says no
- the body keeps the score
- the pathway of surrender
- when things fall apart`,
} as const;

export function getSystemPrompt(mode: 'panel' | 'custom' | 'general' = 'panel'): string {
  return SYSTEM_PROMPTS[mode] || SYSTEM_PROMPTS.panel;
}
```

### Environment Variables
[Source: docs/prd/6-configuration-environment-variables.md]

**Required Environment Variables:**
```bash
# Nous API Configuration
NOUS_API_BASE_URL=https://api.nousresearch.com/v1
NOUS_API_KEY=sk_nous_...                    # From portal.nousresearch.com
HERMES_MODEL=Hermes-4-405B                  # Or Hermes-4-70B for lower cost
BUDDHABOT_MODE=panel                        # Default mode (panel/custom/general)
```

**Configuration in Different Environments:**
- **Local development:** `.env.local` (NOT in git)
- **Vercel production:** Environment Variables in Vercel Dashboard
- **Vercel preview:** Same as production (or separate for testing)

**Fallback Strategy:**
```typescript
const apiBaseUrl = process.env.NOUS_API_BASE_URL || 'https://api.nousresearch.com/v1';
const model = process.env.HERMES_MODEL || 'Hermes-4-405B';
const mode = process.env.BUDDHABOT_MODE || 'panel';
```

### Vercel AI SDK Integration with Custom Nous Provider
[Source: Vercel AI SDK documentation, Nous Research API Oct 2025]

**Complete Implementation Pattern:**
```typescript
// In app/api/chat/route.ts

import { streamText } from 'ai';
import { createOpenAI } from '@ai-sdk/openai';
import { getSystemPrompt } from '@/lib/prompts';

export const runtime = 'edge';

export async function POST(req: Request) {
  try {
    // ... session validation from Story 2.1 ...

    const { messages } = await req.json();

    // Create custom Nous provider
    const nous = createOpenAI({
      baseURL: process.env.NOUS_API_BASE_URL || 'https://api.nousresearch.com/v1',
      apiKey: process.env.NOUS_API_KEY,
    });

    // Get system prompt
    const mode = (process.env.BUDDHABOT_MODE || 'panel') as 'panel';
    const systemPrompt = getSystemPrompt(mode);

    // Stream with Nous Hermes 4
    const result = streamText({
      model: nous(process.env.HERMES_MODEL || 'Hermes-4-405B'),
      system: systemPrompt,  // AI SDK handles system message
      messages,               // User/assistant messages
      temperature: 0.7,
      maxTokens: 2048,
    });

    // AI SDK handles SSE streaming automatically
    return result.toDataStreamResponse();

  } catch (error) {
    console.error('[Chat API Error]', error);
    return new Response(
      JSON.stringify({ error: { code: 'internal_error', message: 'Failed to process request' }}),
      { status: 500, headers: { 'Content-Type': 'application/json' }}
    );
  }
}
```

**Why AI SDK vs. Raw Fetch:**
- ✅ **Cleaner code:** 10 lines vs. 40+ lines
- ✅ **Built-in streaming:** SSE format handled automatically
- ✅ **Type safety:** Full TypeScript support
- ✅ **Error handling:** AI SDK catches common API errors
- ✅ **Assistance UI integration:** `toDataStreamResponse()` works seamlessly
- ✅ **Still uses Nous:** OpenAI-compatible API means Nous works perfectly

### Error Handling & Logging
[Source: docs/architecture/backend-architecture.md]

**Error Mapping Strategy:**
```typescript
if (!response.ok) {
  const errorBody = await response.json().catch(() => ({ message: 'Unknown error' }));

  let userMessage = 'Unable to generate response. Please try again.';

  switch (response.status) {
    case 401:
      userMessage = 'AI service authentication failed';
      console.error('[Nous API] Invalid API key');
      break;
    case 429:
      userMessage = 'Service temporarily busy. Please wait and try again.';
      break;
    case 500:
    case 502:
    case 503:
      userMessage = 'AI service temporarily unavailable';
      break;
  }

  return new Response(
    JSON.stringify({
      error: {
        code: 'upstream_error',
        message: userMessage,
        statusCode: response.status,
      }
    }),
    {
      status: response.status,
      headers: { 'Content-Type': 'application/json' }
    }
  );
}
```

**Request Logging Pattern:**
```typescript
const requestId = crypto.randomUUID(); // Available in Edge runtime

console.log('[Chat API] Request started', {
  requestId,
  userId: session.user.id,
  messageCount: messages.length,
  model: process.env.HERMES_MODEL,
});

// ... make API call ...

console.log('[Nous API] Request sent', {
  requestId,
  endpoint: `${process.env.NOUS_API_BASE_URL}/chat/completions`,
  model: nousRequest.model,
  systemPromptLength: systemPrompt.length,
});
```

### File Locations
[Source: docs/architecture/source-tree.md]

**Files Modified/Created:**
- `lib/prompts.ts` - NEW: System prompt constants and getter function
- `app/api/chat/route.ts` - MODIFY: Add Nous API integration (built on Story 2.1)
- `.env.local.example` - MODIFY: Add Nous API env vars

**Import Paths:**
```typescript
// ✅ Use path alias
import { getSystemPrompt } from '@/lib/prompts';

// ❌ Don't use relative paths
import { getSystemPrompt } from '../../lib/prompts';
```

### Testing

[Source: docs/architecture/testing-strategy.md]

**Manual Testing Checklist:**

1. **Prompt Verification:**
   - [ ] Copy prompt EXACTLY from PRD (including typos)
   - [ ] Verify no modifications made to prompt text
   - [ ] Check prompt is first message in array

2. **API Integration:**
   - [ ] Valid API key → Successful request
   - [ ] Invalid API key → 401 with user-friendly message
   - [ ] Missing API key → Fails gracefully
   - [ ] Correct endpoint URL used
   - [ ] Authorization header set correctly

3. **Request Format:**
   - [ ] Model name matches env var (default: Hermes-4-405B)
   - [ ] Messages array structured correctly
   - [ ] Temperature set to 0.7
   - [ ] max_tokens set to 2048
   - [ ] stream set to true

4. **Error Handling:**
   - [ ] Network errors caught and logged
   - [ ] Upstream errors mapped to user-friendly messages
   - [ ] Request ID included in logs
   - [ ] Error responses structured correctly

**Testing with curl:**
```bash
# Test request format (won't stream yet - Story 2.3)
curl -X POST http://localhost:3000/api/chat \
  -H "Content-Type: application/json" \
  -H "Cookie: authjs.session-token=<your-session>" \
  -d '{
    "messages": [
      {"role": "user", "content": "What is the nature of suffering?"}
    ]
  }' -v
```

**Check Vercel Logs:**
- Request ID logged
- System prompt length logged (verify it's not empty)
- API endpoint logged
- Any errors logged with full context

### Important Notes from Previous Stories

**From Story 2.1 (Chat API Edge Route):**
- Edge runtime configured with `export const runtime = 'edge'`
- Session validation working
- Input validation with Zod schema
- Request/response format established
- Placeholder response currently returned

**Integration Point:**
This story adds the actual AI integration, replacing the placeholder response from Story 2.1. The auth and validation logic remains unchanged.

### Next Steps After This Story

**Story 2.3** will verify:
- Streaming response performance (< 2s TTFB)
- Client-side rendering with Assistance UI
- SSE stream completion handling

**Note:** AI SDK already handles streaming via `toDataStreamResponse()`. Story 2.3 focuses on verification and optimization rather than implementation.

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-08 | 1.0 | Initial story creation | Scrum Master (Bob) |

## Dev Agent Record

### Agent Model Used
claude-sonnet-4-5-20250929

### Debug Log References
- Issue: AI SDK v5 defaults to `/v1/responses` endpoint instead of `/v1/chat/completions`
- Solution: Use `nous.chat(model)` instead of `nous(model)` to force chat completions endpoint
- Reference: AI SDK v5 migration guide - OpenAI provider behavior change

### Completion Notes List
- Created lib/prompts.ts with EXACT system prompt from PRD (including typos)
- Integrated createOpenAI custom provider for Nous Research API
- **CRITICAL FIX**: Used `.chat()` method to force `/chat/completions` endpoint (AI SDK v5 compatibility)
- Added comprehensive error handling with user-friendly messages
- Implemented request logging with unique request IDs
- Fixed TypeScript errors (unused mode parameter in getSystemPrompt)
- Build passes successfully with only minor ESLint warnings
- **VERIFIED WORKING**: Successfully streaming responses from Nous Hermes 4-405B with panel mode

### File List
**Created:**
- `lib/prompts.ts` - System prompt constants and getter function

**Modified:**
- `app/api/chat/route.ts` - Integrated Nous provider with .chat() method, system prompt, error handling, logging

## QA Results
*(To be filled by QA agent after implementation)*
